{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVF0rLLomFFa",
        "outputId": "2e73844b-bfcc-4215-d4da-6d323b23112e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.85)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.7.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.12.0 (from versions: 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.12.0\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install biopython scikit-learn tensorflow==2.12.0 requests tqdm tensorflow-metal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6AegXcLvmOeN"
      },
      "outputs": [],
      "source": [
        "import os, sys, json, math, random\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score\n",
        ")\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "FASTA_PATH = \"Vista_Dataset/vista_sequence.fasta\"\n",
        "RESULT_DIR = \"results\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "SEED = 42\n",
        "EPOCHS = 500      # paper value\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_LEN = 4000   # set None for full length (may be large)\n",
        "QUICK_TEST = False   # True = quick smoke run\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Vista_Dataset/vista_sequence.fasta'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m         records.append({\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: sid, \u001b[33m\"\u001b[39m\u001b[33mspecies\u001b[39m\u001b[33m\"\u001b[39m: species, \u001b[33m\"\u001b[39m\u001b[33msequence\u001b[39m\u001b[33m\"\u001b[39m: seq})\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(records)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df = \u001b[43mload_fasta_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFASTA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sequences from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFASTA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Infer positive/negative if possible\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mload_fasta_records\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_fasta_records\u001b[39m(path):\n\u001b[32m      2\u001b[39m     records = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSeqIO\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfasta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m      4\u001b[39m         sid = rec.id\n\u001b[32m      5\u001b[39m         seq = \u001b[38;5;28mstr\u001b[39m(rec.seq).upper()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/Bio/SeqIO/__init__.py:626\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(handle, format, alphabet)\u001b[39m\n\u001b[32m    624\u001b[39m iterator_generator = _FormatToIterator.get(\u001b[38;5;28mformat\u001b[39m)\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iterator_generator:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m AlignIO._FormatToIterator:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# Use Bio.AlignIO to read in the alignments\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (r \u001b[38;5;28;01mfor\u001b[39;00m alignment \u001b[38;5;129;01min\u001b[39;00m AlignIO.parse(handle, \u001b[38;5;28mformat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m alignment)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/Bio/SeqIO/FastaIO.py:196\u001b[39m, in \u001b[36mFastaIterator.__init__\u001b[39m\u001b[34m(self, source, alphabet)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m alphabet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe alphabet argument is no longer supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFasta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    198\u001b[39m     line = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.stream)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/Bio/SeqIO/Interfaces.py:81\u001b[39m, in \u001b[36mSequenceIterator.__init__\u001b[39m\u001b[34m(self, source, alphabet, fmt)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, _PathLikeTypes):\n\u001b[32m     80\u001b[39m     mode = modes[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     83\u001b[39m     value = source.read(\u001b[32m0\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Vista_Dataset/vista_sequence.fasta'"
          ]
        }
      ],
      "source": [
        "def load_fasta_records(path):\n",
        "    records = []\n",
        "    for rec in SeqIO.parse(path, \"fasta\"):\n",
        "        sid = rec.id\n",
        "        seq = str(rec.seq).upper()\n",
        "        # species label from id prefix (hs / mm)\n",
        "        species = \"human\" if sid.startswith(\"hs\") else \"mouse\" if sid.startswith(\"mm\") else \"unknown\"\n",
        "        records.append({\"id\": sid, \"species\": species, \"sequence\": seq})\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "df = load_fasta_records(FASTA_PATH)\n",
        "print(f\"Loaded {len(df)} sequences from {FASTA_PATH}\")\n",
        "\n",
        "# Infer positive/negative if possible\n",
        "df[\"status\"] = \"positive\"  # if your FASTA only contains enhancers; adjust if negatives exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "SCENARIO1_PARAMS = {\n",
        "    # Predict: human vs mouse (only enhancer sequences)\n",
        "    \"bi_lstm_units\": [256, 128, 64],\n",
        "    \"dropouts\": [0.15, 0.20, 0.20],\n",
        "    \"dense_units\": [512, 256, 128],\n",
        "    \"activation\": \"selu\",\n",
        "    \"output_activation\": \"sigmoid\",\n",
        "    \"loss\": \"binary_crossentropy\",\n",
        "    \"optimizer\": tf.keras.optimizers.RMSprop(),\n",
        "    \"epochs\": 500,\n",
        "    \"batch_size\": 32,\n",
        "    \"scenario\": 1,\n",
        "    \"multiclass\": False\n",
        "}\n",
        "MAX_LEN_CAP = 1000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_vista_fasta(fasta_path):\n",
        "    \"\"\"\n",
        "    Parse FASTA; return list of dicts with fields: seq_id, seq, species, is_enhancer (True/False), label_text\n",
        "    The VISTA FASTA header format varies; this function uses heuristics to extract species and enhancer status.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    # Accept zipped directories as well: find .fa/.fasta files in parent dir\n",
        "    if fasta_path.is_dir():\n",
        "        fasta_files = list(fasta_path.glob(\"*.fa\")) + list(fasta_path.glob(\"*.fasta\")) + list(fasta_path.glob(\"*.txt\"))\n",
        "        if not fasta_files:\n",
        "            raise FileNotFoundError(\"No fasta files found in dir: \" + str(fasta_path))\n",
        "        fasta_path = fasta_files[0]\n",
        "\n",
        "    print(\"Parsing FASTA:\", fasta_path)\n",
        "    for rec in SeqIO.parse(str(fasta_path), \"fasta\"):\n",
        "        header = rec.description\n",
        "        seq = str(rec.seq).upper()\n",
        "        # heuristic species detection\n",
        "        species = None\n",
        "        if re.search(r'\\b(mouse|mus|mm|mouse)\\b', header, re.I):\n",
        "            species = \"mouse\"\n",
        "        if re.search(r'\\b(human|hs|hg|homo sapiens)\\b', header, re.I):\n",
        "            species = \"human\"\n",
        "        # enhancer label detection; VISTA often tags \"positive\" or \"negative\" or \"enhancer\"\n",
        "        is_enhancer = None\n",
        "        if re.search(r'\\b(positive|enhancer|activity|yes|pos)\\b', header, re.I):\n",
        "            is_enhancer = True\n",
        "        if re.search(r'\\b(negative|no|neg|not enhancer)\\b', header, re.I):\n",
        "            is_enhancer = False\n",
        "        # fallback: if neither, attempt to parse metadata after '|' or ';'\n",
        "        rec_id = rec.id\n",
        "        records.append({\n",
        "            \"seq_id\": rec_id,\n",
        "            \"header\": header,\n",
        "            \"seq\": seq,\n",
        "            \"species\": species,\n",
        "            \"is_enhancer\": is_enhancer\n",
        "        })\n",
        "    df = pd.DataFrame(records)\n",
        "    print(\"Parsed\", len(df), \"records; summary:\\n\", df[['species','is_enhancer']].agg(lambda s: s.isnull().sum()))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "INT_MAP = {\"A\":1, \"C\":3, \"G\":2, \"T\":4, \"N\":0}\n",
        "ATOMIC_MAP = {\"A\":70, \"C\":58, \"G\":78, \"T\":66, \"N\":0}\n",
        "EIIP_MAP = {\"A\":0.1260, \"C\":0.1340, \"G\":0.0806, \"T\":0.1335, \"N\":0.0}\n",
        "\n",
        "def encode_integer(seq):\n",
        "    return np.array([INT_MAP.get(b, 0) for b in seq], dtype=float)\n",
        "\n",
        "def encode_atomic(seq):\n",
        "    return np.array([ATOMIC_MAP.get(b, 0) for b in seq], dtype=float)\n",
        "\n",
        "def encode_eiip(seq):\n",
        "    return np.array([EIIP_MAP.get(b, 0.0) for b in seq], dtype=float)\n",
        "\n",
        "def encode_bfdna(seq):\n",
        "    # BFDNA: each base value = (count of that base in the sequence) / (sequence length).\n",
        "    seq = seq.upper()\n",
        "    L = len(seq)\n",
        "    counts = Counter(seq)\n",
        "    # handle zero-length (shouldn't happen)\n",
        "    if L == 0:\n",
        "        return np.zeros(0, dtype=float)\n",
        "    # compute values for A,C,G,T\n",
        "    vals = {}\n",
        "    for b in [\"A\",\"C\",\"G\",\"T\"]:\n",
        "        vals[b] = counts.get(b, 0) / L\n",
        "    # map sequence to values\n",
        "    return np.array([vals.get(b, 0.0) for b in seq], dtype=float)\n",
        "\n",
        "ENCODERS = {\n",
        "    \"integer\": encode_integer,\n",
        "    \"atomic\": encode_atomic,\n",
        "    \"eiip\": encode_eiip,\n",
        "    \"bfdna\": encode_bfdna\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_truncate(arr, max_len):\n",
        "    if len(arr) >= max_len:\n",
        "        return arr[:max_len]\n",
        "    else:\n",
        "        pad = np.zeros(max_len - len(arr), dtype=float)\n",
        "        return np.concatenate([arr, pad])\n",
        "\n",
        "# Min-max scaling across dataset (per position). We'll stack sequences into shape (N, L).\n",
        "def minmax_scale_dataset(X):\n",
        "    # X: numpy array shape (N, L)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    # fit per-column\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_bilstm_model(input_len, params, num_classes=1):\n",
        "    inp = layers.Input(shape=(input_len, 1))\n",
        "    x = inp\n",
        "    # Stack BiLSTM layers as specified. Use return_sequences=True for intermediate layers.\n",
        "    for i, units in enumerate(params[\"bi_lstm_units\"]):\n",
        "        return_seq = True if i < (len(params[\"bi_lstm_units\"]) - 1) else False\n",
        "        x = layers.Bidirectional(layers.LSTM(units, activation=params[\"activation\"], return_sequences=return_seq))(x)\n",
        "        do = params[\"dropouts\"][i] if i < len(params[\"dropouts\"]) else 0.2\n",
        "        if do > 0:\n",
        "            x = layers.Dropout(do)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    for u in params[\"dense_units\"]:\n",
        "        x = layers.Dense(u, activation=params[\"activation\"])(x)\n",
        "    if params[\"multiclass\"]:\n",
        "        out = layers.Dense(num_classes, activation=params[\"output_activation\"])(x)\n",
        "    else:\n",
        "        out = layers.Dense(1, activation=params[\"output_activation\"])(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=params[\"optimizer\"], loss=params[\"loss\"], metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classification_success_index(y_true, y_pred):\n",
        "    # CSI = Precision + TPR - 1\n",
        "    tpr = recall_score(y_true, y_pred, zero_division=0)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    return prec + tpr - 1\n",
        "\n",
        "def g_mean(y_true, y_pred):\n",
        "    # G-mean = sqrt(recall * specificity)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape == (2,2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        return math.sqrt(recall * specificity)\n",
        "    else:\n",
        "        # multiclass: geometric mean of per-class recall\n",
        "        recalls = []\n",
        "        for i in range(len(cm)):\n",
        "            tp = cm[i,i]\n",
        "            fn = cm[i,:].sum() - tp\n",
        "            recall_i = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            recalls.append(recall_i)\n",
        "        prod = 1.0\n",
        "        for r in recalls: prod *= max(r, 1e-12)\n",
        "        return prod ** (1.0/len(recalls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(df, encoder_name=\"bfdna\", scenario=1, max_len_cap=MAX_LEN_CAP):\n",
        "    encoder = ENCODERS[encoder_name]\n",
        "    # Decide which rows to include based on scenario:\n",
        "    # scenario 1 -> only enhancer sequences and species labels (human/mouse) -> binary\n",
        "    # scenario 2 -> multiclass: human_enhancer, mouse_enhancer, no_enhancer\n",
        "    df2 = df.copy()\n",
        "    # Ensure species/is_enhancer are not None: try to infer from header if missing\n",
        "    # (We already did heuristics in parse)\n",
        "    if scenario == 1:\n",
        "        # keep only rows where is_enhancer is True and species is known\n",
        "        df_filt = df2[(df2[\"is_enhancer\"]==True) & (df2[\"species\"].notnull())].reset_index(drop=True)\n",
        "        y = df_filt[\"species\"].map(lambda s: 1 if s==\"human\" else 0).values  # human=1, mouse=0\n",
        "    else:\n",
        "        # multiclass\n",
        "        # three classes: 'human_enhancer', 'mouse_enhancer', 'no_enhancer'\n",
        "        def map_label(row):\n",
        "            if row[\"is_enhancer\"]==True and row[\"species\"]==\"human\":\n",
        "                return \"human_enhancer\"\n",
        "            if row[\"is_enhancer\"]==True and row[\"species\"]==\"mouse\":\n",
        "                return \"mouse_enhancer\"\n",
        "            return \"no_enhancer\"\n",
        "        df_filt = df2.copy()\n",
        "        df_filt[\"label\"] = df_filt.apply(map_label, axis=1)\n",
        "        y = df_filt[\"label\"].values\n",
        "\n",
        "    # Encode sequences:\n",
        "    encoded = []\n",
        "    lengths = []\n",
        "    for seq in df_filt[\"seq\"].values:\n",
        "        arr = encoder(seq)\n",
        "        lengths.append(len(arr))\n",
        "        encoded.append(arr)\n",
        "    if len(encoded) == 0:\n",
        "        raise RuntimeError(\"No sequences found after filtering — check dataset parsing/headers.\")\n",
        "\n",
        "    # Determine max_len: min(max(lengths), max_len_cap)\n",
        "    dataset_max_len = max(lengths)\n",
        "    max_len = min(dataset_max_len, max_len_cap)\n",
        "    print(f\"Dataset max length = {dataset_max_len}; using max_len = {max_len}\")\n",
        "\n",
        "    # Pad/truncate and stack\n",
        "    X = np.stack([pad_truncate(arr, max_len) for arr in encoded], axis=0)  # shape (N, L)\n",
        "    # Min-max normalization across dataset (per position)\n",
        "    X_scaled, scaler = minmax_scale_dataset(X)\n",
        "    # Expand dims for model input: (N, L, 1)\n",
        "    X_scaled = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "    return X_scaled, y, df_filt, scaler, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(X, y, params, df_meta, label_names=None):\n",
        "    # split 75% train, 15% val, 15% test (paper)\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "    train_end = int(0.75 * N)\n",
        "    val_end = int(0.90 * N)\n",
        "    train_idx, val_idx, test_idx = idx[:train_end], idx[train_end:val_end], idx[val_end:]\n",
        "    X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
        "    y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
        "\n",
        "    # Prepare labels\n",
        "    if params[\"multiclass\"]:\n",
        "        # y are strings -> one-hot encode\n",
        "        lb = LabelBinarizer()\n",
        "        lb.fit(y)  # fits unique labels\n",
        "        y_train_o = lb.transform(y_train)\n",
        "        y_val_o = lb.transform(y_val)\n",
        "        y_test_o = lb.transform(y_test)\n",
        "        num_classes = y_train_o.shape[1]\n",
        "    else:\n",
        "        # y are 0/1\n",
        "        y_train_o, y_val_o, y_test_o = y_train.astype(int), y_val.astype(int), y_test.astype(int)\n",
        "        num_classes = 1\n",
        "\n",
        "    model = build_bilstm_model(input_len=X.shape[1], params=params, num_classes=num_classes)\n",
        "\n",
        "    # Callbacks: save best model\n",
        "    ckpt_path = MODEL_DIR / f\"scenario{params['scenario']}_{random.randint(0,9999)}.h5\"\n",
        "    cb = [\n",
        "        callbacks.ModelCheckpoint(str(ckpt_path), monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "        # optional early stopping (not in paper) — commented here; you can enable if desired.\n",
        "        # callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(X_train, y_train_o, validation_data=(X_val, y_val_o),\n",
        "                        epochs=params[\"epochs\"], batch_size=params[\"batch_size\"], callbacks=cb, verbose=2)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_weights(str(ckpt_path))\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    if params[\"multiclass\"]:\n",
        "        y_pred_idx = np.argmax(y_pred_prob, axis=1)\n",
        "        # map back to labels\n",
        "        if isinstance(y_test_o, np.ndarray):\n",
        "            # decode using LabelBinarizer\n",
        "            # We need lb stored earlier; re-fit to all labels to get mapping\n",
        "            lb2 = LabelBinarizer(); lb2.fit(y)\n",
        "            label_list = lb2.classes_\n",
        "            y_test_decoded = y_test\n",
        "            y_pred_decoded = label_list[y_pred_idx]\n",
        "        else:\n",
        "            raise RuntimeError(\"Unexpected type for multiclass y\")\n",
        "    else:\n",
        "        y_pred_prob = y_pred_prob.ravel()\n",
        "        y_pred_bin = (y_pred_prob >= 0.5).astype(int)\n",
        "        y_test_decoded = y_test\n",
        "        y_pred_decoded = y_pred_bin\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = {}\n",
        "    if params[\"multiclass\"]:\n",
        "        # For multiclass AUC we compute macro-average via one-vs-rest if possible\n",
        "        lb2 = LabelBinarizer(); lb2.fit(y)\n",
        "        y_test_bin = lb2.transform(y_test)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_test_bin, y_pred_prob, average=\"macro\", multi_class=\"ovr\")\n",
        "        except Exception:\n",
        "            auc = float(\"nan\")\n",
        "        metrics[\"AUC_macro\"] = auc\n",
        "        # Accuracy etc using decoded labels\n",
        "        acc = accuracy_score(y_test_decoded, y_pred_decoded)\n",
        "        prec = precision_score(y_test_decoded, y_pred_decoded, average=\"macro\", zero_division=0)\n",
        "        rec = recall_score(y_test_decoded, y_pred_decoded, average=\"macro\", zero_division=0)\n",
        "        f1 = f1_score(y_test_decoded, y_pred_decoded, average=\"macro\", zero_division=0)\n",
        "        metrics.update({\"accuracy\":acc, \"precision_macro\":prec, \"recall_macro\":rec, \"f1_macro\":f1})\n",
        "        # For per-class metrics you can compute separately\n",
        "    else:\n",
        "        auc = roc_auc_score(y_test, y_pred_prob) if len(np.unique(y_test)) == 2 else float(\"nan\")\n",
        "        metrics[\"AUC\"] = auc\n",
        "        metrics[\"accuracy\"] = accuracy_score(y_test, y_pred_bin)\n",
        "        metrics[\"precision\"] = precision_score(y_test, y_pred_bin, zero_division=0)\n",
        "        metrics[\"recall\"] = recall_score(y_test, y_pred_bin, zero_division=0)\n",
        "        metrics[\"f1\"] = f1_score(y_test, y_pred_bin, zero_division=0)\n",
        "        metrics[\"CSI\"] = classification_success_index(y_test, y_pred_bin)\n",
        "        metrics[\"Gmean\"] = g_mean(y_test, y_pred_bin)\n",
        "        metrics[\"MCC\"] = matthews_corrcoef(y_test, y_pred_bin)\n",
        "        metrics[\"Kappa\"] = cohen_kappa_score(y_test, y_pred_bin)\n",
        "\n",
        "    print(\"Evaluation metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    # Save predictions CSV\n",
        "    out_df = df_meta.iloc[test_idx].copy()\n",
        "    out_df = out_df.reset_index(drop=True)\n",
        "    if params[\"multiclass\"]:\n",
        "        out_df[\"true_label\"] = y_test\n",
        "        out_df[\"pred_label\"] = y_pred_decoded\n",
        "        # store per-class probs\n",
        "        for i,lab in enumerate(lb2.classes_):\n",
        "            out_df[f\"prob_{lab}\"] = y_pred_prob[:,i]\n",
        "    else:\n",
        "        out_df[\"true_label\"] = y_test\n",
        "        out_df[\"pred_label\"] = y_pred_decoded\n",
        "        out_df[\"prob_positive\"] = y_pred_prob\n",
        "    out_csv = RESULTS_DIR / f\"predictions_scenario{params['scenario']}.csv\"\n",
        "    out_df.to_csv(out_csv, index=False)\n",
        "    print(\"Saved predictions to\", out_csv)\n",
        "\n",
        "    # Save metrics\n",
        "    with open(RESULTS_DIR / f\"metrics_scenario{params['scenario']}.json\", \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    print(\"Saved metrics.\")\n",
        "\n",
        "    return model, history, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing FASTA: Vista_Dataset/vista_sequences.fasta\n",
            "Parsed 3408 records; summary:\n",
            " species        0\n",
            "is_enhancer    0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "==============================\n",
            "Encoder: bfdna\n",
            "==============================\n",
            "Scenario 1 failed for encoder bfdna : name 'prepare_dataset' is not defined\n",
            "Scenario 2 failed for encoder bfdna : name 'prepare_dataset' is not defined\n",
            "\n",
            "\n",
            "==============================\n",
            "Encoder: eiip\n",
            "==============================\n",
            "Scenario 1 failed for encoder eiip : name 'prepare_dataset' is not defined\n",
            "Scenario 2 failed for encoder eiip : name 'prepare_dataset' is not defined\n",
            "\n",
            "\n",
            "==============================\n",
            "Encoder: atomic\n",
            "==============================\n",
            "Scenario 1 failed for encoder atomic : name 'prepare_dataset' is not defined\n",
            "Scenario 2 failed for encoder atomic : name 'prepare_dataset' is not defined\n",
            "\n",
            "\n",
            "==============================\n",
            "Encoder: integer\n",
            "==============================\n",
            "Scenario 1 failed for encoder integer : name 'prepare_dataset' is not defined\n",
            "Scenario 2 failed for encoder integer : name 'prepare_dataset' is not defined\n",
            "All done. Results saved to results\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 1) Download dataset (may throw if fails)\n",
        "    try:\n",
        "        downloaded = FASTA_FILE\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Automatic download failed. Please manually download the VISTA FASTA from https://enhancer.lbl.gov/vista/downloads and upload to `vista_data/` then re-run.\")\n",
        "\n",
        "    # 2) Parse FASTA\n",
        "    df = parse_vista_fasta(downloaded)\n",
        "\n",
        "    # Quick cleanup heuristics: if there are ambiguous 'N' categories or missing labels,\n",
        "    # user might need to manually map. We'll proceed with available labels as best-effort.\n",
        "\n",
        "    # 3) For each encoder and scenario, prepare dataset, train, evaluate\n",
        "    encoders_to_try = [\"bfdna\", \"eiip\", \"atomic\", \"integer\"]  # order matches paper\n",
        "    results_summary = {}\n",
        "    for encoder_name in encoders_to_try:\n",
        "        print(\"\\n\\n==============================\")\n",
        "        print(\"Encoder:\", encoder_name)\n",
        "        print(\"==============================\")\n",
        "        # Scenario 1\n",
        "        try:\n",
        "            X1, y1, df_meta1, scaler1, max_len1 = prepare_dataset(df, encoder_name=encoder_name, scenario=1)\n",
        "            model1, hist1, metrics1 = train_and_evaluate(X1, y1, SCENARIO1_PARAMS, df_meta1)\n",
        "            results_summary[f\"{encoder_name}_scenario1\"] = metrics1\n",
        "        except Exception as e:\n",
        "            print(\"Scenario 1 failed for encoder\", encoder_name, \":\", e)\n",
        "\n",
        "        # Scenario 2\n",
        "        try:\n",
        "            X2, y2, df_meta2, scaler2, max_len2 = prepare_dataset(df, encoder_name=encoder_name, scenario=2)\n",
        "            model2, hist2, metrics2 = train_and_evaluate(X2, y2, SCENARIO2_PARAMS, df_meta2)\n",
        "            results_summary[f\"{encoder_name}_scenario2\"] = metrics2\n",
        "        except Exception as e:\n",
        "            print(\"Scenario 2 failed for encoder\", encoder_name, \":\", e)\n",
        "\n",
        "    # Save summary\n",
        "    with open(RESULTS_DIR / \"summary_results.json\", \"w\") as f:\n",
        "        json.dump(results_summary, f, indent=2)\n",
        "    print(\"All done. Results saved to\", RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
